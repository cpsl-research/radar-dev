{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary packages\n",
    "import torch\n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# base path of the dataset\n",
    "from pyimagesearch.model import UNet\n",
    "DATASET_PATH = \"/data/DeepSense6G/stationary_scenarios/scenario31/generated_dataset\"\n",
    "\n",
    "# define the path to the images and masks dataset\n",
    "IMAGE_DATASET_PATH = os.path.join(DATASET_PATH, \"radar\")\n",
    "MASK_DATASET_PATH = os.path.join(DATASET_PATH, \"lidar\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the test split\n",
    "TEST_SPLIT = 0.15\n",
    "\n",
    "# determine the device to be used for training and evaluation\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# determine if we will be pinning memory during data loading\n",
    "PIN_MEMORY = True if DEVICE == \"cuda\" else False\n",
    "\n",
    "# define the number of channels in the input, number of classes,\n",
    "# and number of levels in the U-Net model\n",
    "NUM_CHANNELS = 1\n",
    "NUM_CLASSES = 1\n",
    "NUM_LEVELS = 3\n",
    "# initialize learning rate, number of epochs to train for, and the\n",
    "# batch size\n",
    "INIT_LR = 0.001\n",
    "NUM_EPOCHS = 40\n",
    "BATCH_SIZE = 64\n",
    "# define the input image dimensions\n",
    "INPUT_IMAGE_WIDTH = 256\n",
    "INPUT_IMAGE_HEIGHT = 256\n",
    "# define threshold to filter weak predictions\n",
    "THRESHOLD = 0.5\n",
    "# define the path to the base output directory\n",
    "BASE_OUTPUT = \"examples/PyTorch_tutorial/output\"\n",
    "# define the path to the output serialized model, model training\n",
    "# plot, and testing image paths\n",
    "# MODEL_PATH = os.path.join(BASE_OUTPUT, \"unet_tgs_salt.pth\")\n",
    "# PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])\n",
    "# TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "\tdef __init__(self, imagePaths, maskPaths, transforms):\n",
    "\t\t# store the image and mask filepaths, and augmentation\n",
    "\t\t# transforms\n",
    "\t\tself.imagePaths = imagePaths\n",
    "\t\tself.maskPaths = maskPaths\n",
    "\t\tself.transforms = transforms\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\t# return the number of total samples contained in the dataset\n",
    "\t\treturn len(self.imagePaths)\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t# grab the image path from the current index\n",
    "\t\timagePath = self.imagePaths[idx]\n",
    "\t\tmaskPath = self.maskPaths[idx]\n",
    "\t\t# load the image from disk, swap its channels from BGR to RGB,\n",
    "\t\t# and read the associated mask from disk in grayscale mode\n",
    "\t\timage = np.load(imagePath)\n",
    "\t\t\n",
    "\t\tmask = np.load(maskPath)\n",
    "\t\t\n",
    "\t\t# check to see if we are applying any transformations\n",
    "\t\tif self.transforms is not None:\n",
    "\t\t\t# apply the transformations to both image and its mask\n",
    "\t\t\timage = self.transforms(image)\n",
    "\t\t\tmask = self.transforms(mask)\n",
    "\t\t\t\n",
    "\t\t# return a tuple of the image and its mask\n",
    "\t\treturn (image, mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "\n",
    "def list_files(directory, extension):\n",
    "    return (os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(extension))\n",
    "\n",
    "\n",
    "directory_im = IMAGE_DATASET_PATH\n",
    "directory_mas = MASK_DATASET_PATH\n",
    "extension = '.npy'\n",
    "\n",
    "\n",
    "imagePaths = sorted(list_files(directory_im, extension))\n",
    "maskPaths = sorted(list_files(directory_mas, extension))\n",
    "\n",
    "split = train_test_split(imagePaths, maskPaths,\n",
    "\ttest_size=0.15, random_state=42)\n",
    "\n",
    "(trainImages, testImages) = split[:2]\n",
    "(trainMasks, testMasks) = split[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((256,256))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDS = SegmentationDataset(imagePaths=trainImages, maskPaths=trainMasks,\n",
    "\ttransforms=transforms)\n",
    "testDS = SegmentationDataset(imagePaths=testImages, maskPaths=testMasks,\n",
    "    transforms=transforms)\n",
    "trainDS[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] found 5960 examples in the training set...\n",
      "[INFO] found 1052 examples in the test set...\n"
     ]
    }
   ],
   "source": [
    "from pyimagesearch import config\n",
    "print(f\"[INFO] found {len(trainDS)} examples in the training set...\")\n",
    "print(f\"[INFO] found {len(testDS)} examples in the test set...\")\n",
    "\n",
    "trainLoader = DataLoader(trainDS, shuffle=True,\n",
    "\tbatch_size=config.BATCH_SIZE, pin_memory=config.PIN_MEMORY,\n",
    "\tnum_workers=os.cpu_count())\n",
    "testLoader = DataLoader(testDS, shuffle=False,\n",
    "\tbatch_size=config.BATCH_SIZE, pin_memory=config.PIN_MEMORY,\n",
    "\tnum_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "unet = UNet().to(config.DEVICE)\n",
    "\n",
    "# initialize loss function and optimizer\n",
    "lossFunc = BCEWithLogitsLoss()\n",
    "opt = Adam(unet.parameters(), lr=config.INIT_LR)\n",
    "\n",
    "# calculate steps per epoch for training and test set\n",
    "trainSteps = len(trainDS) // config.BATCH_SIZE\n",
    "testSteps = len(testDS) // config.BATCH_SIZE\n",
    "H = {\"train_loss\": [], \"test_loss\": []}\n",
    "\n",
    "# loop over epochs\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 7.78 GiB total capacity; 6.02 GiB already allocated; 37.00 MiB free; 6.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m unet(x)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/avdev-sandbox-VTVXZ3G0-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/radar-dev/examples/PyTorch_tutorial/pyimagesearch/model.py:111\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m encFeatures \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x)\n\u001b[1;32m    109\u001b[0m \u001b[39m# pass the encoder features through decoder making sure that\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m# their dimensions are suited for concatenation\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m decFeatures \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(encFeatures[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    112\u001b[0m     encFeatures[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][\u001b[39m1\u001b[39;49m:])\n\u001b[1;32m    114\u001b[0m \u001b[39m# pass the decoder features through the regression head to\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m# obtain the segmentation mask\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39mmap\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead(decFeatures)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/avdev-sandbox-VTVXZ3G0-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/radar-dev/examples/PyTorch_tutorial/pyimagesearch/model.py:67\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, encFeatures)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, encFeatures):\n\u001b[1;32m     64\u001b[0m \t\u001b[39m# loop through the number of channels\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \t\u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannels) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     66\u001b[0m \t\t\u001b[39m# pass the inputs through the upsampler blocks\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \t\tx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupconvs[i](x)\n\u001b[1;32m     69\u001b[0m \t\t\u001b[39m# crop the current features from the encoder blocks,\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \t\t\u001b[39m# concatenate them with the current upsampled features,\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \t\t\u001b[39m# and pass the concatenated output through the current\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \t\t\u001b[39m# decoder block\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \t\tencFeat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrop(encFeatures[i], x)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/avdev-sandbox-VTVXZ3G0-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/avdev-sandbox-VTVXZ3G0-py3.8/lib/python3.8/site-packages/torch/nn/modules/conv.py:923\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[39m# One cannot replace List by Tuple or Sequence in \"_output_padding\" because\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \u001b[39m# TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.\u001b[39;00m\n\u001b[1;32m    920\u001b[0m output_padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_padding(\n\u001b[1;32m    921\u001b[0m     \u001b[39minput\u001b[39m, output_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 923\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv_transpose2d(\n\u001b[1;32m    924\u001b[0m     \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    925\u001b[0m     output_padding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 7.78 GiB total capacity; 6.02 GiB already allocated; 37.00 MiB free; 6.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "unet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([64, 1, 256, 256])) must be the same as input size (torch.Size([64, 1, 128, 128]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# perform a forward pass and calculate the training loss\u001b[39;00m\n\u001b[1;32m     16\u001b[0m pred \u001b[39m=\u001b[39m unet(x)\n\u001b[0;32m---> 17\u001b[0m loss \u001b[39m=\u001b[39m lossFunc(pred, y)\n\u001b[1;32m     19\u001b[0m \u001b[39m# first, zero out any previously accumulated gradients, then\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# perform backpropagation, and then update model parameters\u001b[39;00m\n\u001b[1;32m     21\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/avdev-sandbox-VTVXZ3G0-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/avdev-sandbox-VTVXZ3G0-py3.8/lib/python3.8/site-packages/torch/nn/modules/loss.py:704\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 704\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[1;32m    705\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    706\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[1;32m    707\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/avdev-sandbox-VTVXZ3G0-py3.8/lib/python3.8/site-packages/torch/nn/functional.py:2980\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2977\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   2979\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[0;32m-> 2980\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()))\n\u001b[1;32m   2982\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([64, 1, 256, 256])) must be the same as input size (torch.Size([64, 1, 128, 128]))"
     ]
    }
   ],
   "source": [
    "for e in tqdm(range(config.NUM_EPOCHS)):\n",
    "\t# set the model in training mode\n",
    "\tunet.train()\n",
    "\t\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalTestLoss = 0\n",
    "\t\n",
    "\t# loop over the training set\n",
    "\tfor (i, (x, y)) in enumerate(trainLoader):\n",
    "\t\t\n",
    "\t\t# send the input to the device\n",
    "\t\t(x, y) = (x.to(config.DEVICE).float(), y.to(config.DEVICE).float())\n",
    "\t\t\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "\t\tpred = unet(x)\n",
    "\t\tloss = lossFunc(pred, y)\n",
    "\t\t\n",
    "\t\t# first, zero out any previously accumulated gradients, then\n",
    "\t\t# perform backpropagation, and then update model parameters\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\t\t\n",
    "\t\t# add the loss to the total training loss so far\n",
    "\t\ttotalTrainLoss += loss\n",
    "\t\t\n",
    "\t# switch off autograd\n",
    "\twith torch.no_grad():\n",
    "\t\t# set the model in evaluation mode\n",
    "\t\tunet.eval()\n",
    "\t\t\n",
    "\t\t# loop over the validation set\n",
    "\t\tfor (x, y) in testLoader:\n",
    "\t\t\t\n",
    "\t\t\t# send the input to the device\n",
    "\t\t\t(x, y) = (x.to(config.DEVICE), y.to(config.DEVICE))\n",
    "\t\t\t\n",
    "\t\t\t# make the predictions and calculate the validation loss\n",
    "\t\t\tpred = unet(x)\n",
    "\t\t\ttotalTestLoss += lossFunc(pred, y)\n",
    "\t\t\t\n",
    "\t# calculate the average training and validation loss\n",
    "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\tavgTestLoss = totalTestLoss / testSteps\n",
    "\t\n",
    "\t# update our training history\n",
    "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n",
    "\t\n",
    "\t# print the model training and validation information\n",
    "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, config.NUM_EPOCHS))\n",
    "\tprint(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(\n",
    "\t\tavgTrainLoss, avgTestLoss))\n",
    "\t\n",
    "# display the total time needed to perform the training\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "\tendTime - startTime))\n",
    "\n",
    "# plot the training loss\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"test_loss\"], label=\"test_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(config.PLOT_PATH)\n",
    "# serialize the model to disk\n",
    "torch.save(unet, config.MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = DataLoader(trainDS, shuffle=True,\n",
    "\tbatch_size=6, pin_memory=PIN_MEMORY,\n",
    "\tnum_workers=os.cpu_count())\n",
    "testLoader = DataLoader(testDS, shuffle=False,\n",
    "\tbatch_size=6, pin_memory=PIN_MEMORY,\n",
    "\tnum_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = trainLoader.dataset\n",
    "\n",
    "\n",
    "\n",
    "single_example = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, classes = next(iter(trainLoader)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avdev-sandbox-VTVXZ3G0-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
